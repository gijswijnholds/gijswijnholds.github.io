---
title: "Improving BERT Pretraining with Syntactic Supervision"
collection: publications
permalink: /publication/2021-04-21-paper-title-number-16
excerpt: 'We train a BERT model from scratch for Dutch, while incorporating a supertagging objective to induce a syntactic bias. Initial experiments hint at improved or equal performance on a number of tasks, despite pretraining on a small amount of data.'
type: preprint
date: 2021-04-21
year: 2021
venue: 'ArXiV'
paperurl: 'https://arxiv.org/pdf/2104.10516.pdf'
citation: 'Tziafas, G. and Kogkalidis, K. and Wijnholds, G. and Moortgat, M. (2021). &quot;Improving BERT Pretraining with Syntactic Supervision.&quot;  <i>ArXiV preprint</i>.'
---
We train a BERT model from scratch for Dutch, while incorporating a supertagging objective to induce a syntactic bias. Initial experiments hint at improved or equal performance on a number of tasks, despite pretraining on a small amount of data.
